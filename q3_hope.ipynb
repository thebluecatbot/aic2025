{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db876fcb",
   "metadata": {},
   "source": [
    "# Setup: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e109f8-05e8-4c9e-b598-83b5a1b048b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/sriya/gpu-env/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /home/sriya/gpu-env/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in /home/sriya/gpu-env/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from triton==3.3.0->torch) (80.3.1)\n",
      "Requirement already satisfied: numpy in /home/sriya/gpu-env/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: transformers in /home/sriya/gpu-env/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: datasets in /home/sriya/gpu-env/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/sriya/gpu-env/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: PyPDF2 in /home/sriya/gpu-env/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: pdfplumber in /home/sriya/gpu-env/lib/python3.10/site-packages (0.11.6)\n",
      "Requirement already satisfied: sentence-transformers in /home/sriya/gpu-env/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/sriya/gpu-env/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /home/sriya/gpu-env/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: tqdm in /home/sriya/gpu-env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfplumber) (11.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfminer.six==20250327->pdfplumber) (45.0.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/sriya/gpu-env/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/sriya/gpu-env/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers) (80.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install transformers datasets faiss-cpu PyPDF2 pdfplumber sentence-transformers scikit-learn matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c9344e-b4a5-4d37-982a-078c96bd107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/sriya/gpu-env/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /home/sriya/gpu-env/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in /home/sriya/gpu-env/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from triton==3.3.0->torch) (80.3.1)\n",
      "Requirement already satisfied: numpy in /home/sriya/gpu-env/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: transformers in /home/sriya/gpu-env/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: datasets in /home/sriya/gpu-env/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/sriya/gpu-env/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/sriya/gpu-env/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /home/sriya/gpu-env/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: PyPDF2 in /home/sriya/gpu-env/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: pdfplumber in /home/sriya/gpu-env/lib/python3.10/site-packages (0.11.6)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfplumber) (11.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pdfminer.six==20250327->pdfplumber) (45.0.2)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/sriya/gpu-env/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/sriya/gpu-env/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Requirement already satisfied: sentence-transformers in /home/sriya/gpu-env/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (4.38.2)\n",
      "Requirement already satisfied: tqdm in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: numpy in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /home/sriya/gpu-env/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers) (80.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sriya/gpu-env/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in /home/sriya/gpu-env/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /home/sriya/gpu-env/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: tqdm in /home/sriya/gpu-env/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sriya/gpu-env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sriya/gpu-env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# In[1]\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets\n",
    "!pip install faiss-cpu\n",
    "!pip install PyPDF2 pdfplumber\n",
    "!pip install sentence-transformers\n",
    "!pip install scikit-learn matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82955ed5-61e9-49eb-8c46-1929c0ec3acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /home/sriya/gpu-env/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/sriya/gpu-env/lib/python3.10/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/sriya/gpu-env/lib/python3.10/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/sriya/gpu-env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/sriya/gpu-env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sriya/gpu-env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/sriya/gpu-env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/sriya/gpu-env/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3d964",
   "metadata": {},
   "source": [
    "#  Set Up API\n",
    "\n",
    "Here we import `groq` for LLM queries, `sentence-transformers` for embedding, and `pdfplumber` for PDF extraction.  \n",
    "We also configure the GROQ API client.\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814b12d7-6416-4cbb-9180-9c7681068f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GROQ_API_KEY environment variable not set\")\n",
    "client = Groq(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdbd21",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc89ec0-a561-4bec-b64b-a736e28f3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pdfplumber\n",
    "from tiktoken import get_encoding\n",
    "# Initialize embedding model & tokenizer\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "embedder   = SentenceTransformer(EMBED_MODEL)\n",
    "encoder    = get_encoding(\"cl100k_base\")\n",
    "MAX_TOKENS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae716008",
   "metadata": {},
   "source": [
    "# Load and Chunk the PDF Document\n",
    "\n",
    "We use `pdfplumber` to extract the raw text from the PDF.  \n",
    "Then we chunk the text into semantic units (e.g., ~500 tokens each) to prepare for embedding and retrieval.\n",
    "\n",
    "Chunking is essential to keep context manageable and improve retrieval accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e281e3-c401-4f33-b905-78d1bed4e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 2: Loaded 'NIPS-2017-attention-is-all-you-need-Paper.pdf' → 18 chunks (≤500 tokens each).\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 2: PDF LOADING & CHUNKING ─────────────────────────────────────────────\n",
    "def load_pdf(path: str) -> str:\n",
    "    \"\"\"Extract all text from each page of the PDF into one big string.\"\"\"\n",
    "    pages = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                pages.append(text)\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = MAX_TOKENS) -> list[str]:\n",
    "    \"\"\"Split `text` into chunks of up to `max_tokens` tokens each.\"\"\"\n",
    "    tokens = encoder.encode(text)\n",
    "    chunks = [\n",
    "        encoder.decode(tokens[i : i + max_tokens])\n",
    "        for i in range(0, len(tokens), max_tokens)\n",
    "    ]\n",
    "    return chunks\n",
    "\n",
    "# point this to your paper\n",
    "PDF_PATH = \"NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
    "\n",
    "raw_text = load_pdf(PDF_PATH)\n",
    "chunks   = chunk_text(raw_text)\n",
    "\n",
    "print(f\"✅ Cell 2: Loaded '{PDF_PATH}' → {len(chunks)} chunks (≤{MAX_TOKENS} tokens each).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ed2fe",
   "metadata": {},
   "source": [
    "# Embed Chunks and Build Vector Index\n",
    "\n",
    "We use the `all-MiniLM-L6-v2` model from `sentence-transformers` to embed each chunk into vector space.\n",
    "\n",
    "We then build a FAISS index for fast similarity search. This enables efficient retrieval of relevant chunks given a user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab247cc9-4faf-4e37-a431-bf8c2422850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8028238a79d2444d8c8e8d8276a60d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 3: FAISS index built with 18 vectors, dimension=384.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 3: EMBEDDING & FAISS INDEXING ─────────────────────────────────────────\n",
    "# 1) Embed each chunk (this may take a minute)\n",
    "vectors = embedder.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "# 2) Build a flat‐L2 FAISS index and add vectors\n",
    "dim   = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(vectors)\n",
    "\n",
    "print(f\"✅ Cell 3: FAISS index built with {index.ntotal} vectors, dimension={dim}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25e6ef-cee6-4226-a735-b9bad7e7e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response object: ModelListResponse(data=[Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072), Model(id='meta-llama/llama-guard-4-12b', created=1746743847, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=1024), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='playai-tts-arabic', created=1740682783, object='model', owned_by='PlayAI', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='playai-tts', created=1740682771, object='model', owned_by='PlayAI', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='meta-llama/llama-4-maverick-17b-128e-instruct', created=1743877158, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='meta-llama/llama-4-scout-17b-16e-instruct', created=1743874824, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='mistral-saba-24b', created=1739996492, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None, max_completion_tokens=32768), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='compound-beta-mini', created=1742953279, object='model', owned_by='Groq', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=32768), Model(id='allam-2-7b', created=1737672203, object='model', owned_by='SDAIA', active=True, context_window=4096, public_apps=None, max_completion_tokens=4096), Model(id='compound-beta', created=1740880017, object='model', owned_by='Groq', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='qwen-qwq-32b', created=1741214760, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072)], object='list')\n",
      "Attributes on resp: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'construct', 'copy', 'data', 'dict', 'from_orm', 'json', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'object', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_dict', 'to_json', 'update_forward_refs', 'validate'] \n",
      "\n",
      "\n",
      "Found attribute 'data' (type=<class 'list'>), first 5 entries:\n",
      "  Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None, max_completion_tokens=448)\n",
      "  Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072)\n",
      "  Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072)\n",
      "  Model(id='meta-llama/llama-guard-4-12b', created=1746743847, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=1024)\n",
      "  Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resp = client.models.list()\n",
    "print(\"Response object:\", resp)\n",
    "print(\"Attributes on resp:\", dir(resp), \"\\n\")\n",
    "\n",
    "# If there's a .data or .models attribute, inspect it:\n",
    "for attr in (\"data\", \"models\", \"_data\", \"_models\"):\n",
    "    if hasattr(resp, attr):\n",
    "        lst = getattr(resp, attr)\n",
    "        print(f\"\\nFound attribute '{attr}' (type={type(lst)}), first 5 entries:\")\n",
    "        for e in lst[:5]:\n",
    "            print(\" \", e)\n",
    "        break\n",
    "else:\n",
    "    # Fallback: iterate resp directly\n",
    "    print(\"\\nFalling back to iterating resp directly:\")\n",
    "    for i, e in enumerate(resp):\n",
    "        if i >= 5: break\n",
    "        print(\" \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd87e208-53bd-4f63-8d0e-b9a0e57f3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• data\n",
      "• object\n"
     ]
    }
   ],
   "source": [
    "for name, *_ in models:\n",
    "    print(\"•\", name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d69d87",
   "metadata": {},
   "source": [
    "# Define RAG Query Function\n",
    "\n",
    "This function performs Retrieval-Augmented Generation:\n",
    "1. Embeds the user query.\n",
    "2. Searches FAISS for top-k similar chunks.\n",
    "3. Concatenates the chunks into a context.\n",
    "4. Sends a prompt to the GROQ LLM to generate an answer.\n",
    "\n",
    "This is the core of our QA system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8129f16-d0d2-458c-a3fd-00e856f8fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: RETRIEVAL & RAG ANSWERING ────────────────────────────────────────\n",
    "\n",
    "def retrieve(query: str, k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode `query`, search FAISS for top-k similar chunks,\n",
    "    and return those text chunks.\n",
    "    \"\"\"\n",
    "    q_vec       = embedder.encode([query])\n",
    "    distances, I = index.search(q_vec, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def make_prompt(context: list[str], question: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a “don’t hallucinate” prompt from retrieved context.\n",
    "    \"\"\"\n",
    "    header = (\n",
    "        \"You are a factual, context-grounded assistant. \"\n",
    "        \"Use ONLY the following excerpts (do not hallucinate):\"\n",
    "    )\n",
    "    ctxt = \"\\n\".join(f\"- {c}\" for c in context)\n",
    "    return f\"{header}\\n\\n{ctxt}\\n\\nQuestion: {question}\\n\"\n",
    "\n",
    "# ── Cell 4: RETRIEVAL & RAG ANSWERING (using compound-beta-mini) ─────────────\n",
    "import numpy as np\n",
    "\n",
    "def answer_question(question: str, top_k: int = 5) -> str:\n",
    "    # 1) Encode and retrieve\n",
    "    q_vecs = embedder.encode([question])  # shape=(1, dim)\n",
    "    distances, indices = index.search(q_vecs.astype(\"float32\"), top_k)\n",
    "    context_blocks = [chunks[i] for i in indices[0]]\n",
    "\n",
    "    # 2) Build chat messages\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a factual, context-grounded assistant. \"\n",
    "            \"Answer using ONLY the provided context; do not hallucinate.\"\n",
    "        )\n",
    "    }\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Here are the context excerpts:\\n\"\n",
    "            + \"\\n\".join(f\"- {c}\" for c in context_blocks)\n",
    "            + f\"\\n\\nQuestion: {question}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 3) Call Groq Chat Completions on compound-beta-mini\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"compound-beta-mini\",\n",
    "        messages=[system_msg, user_msg],\n",
    "        max_tokens=256,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6251f376-8efc-4ced-a8b8-100694cac652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the self-attention mechanism about?\n",
      "A: An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, and values come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 5: TEST THE PIPELINE ────────────────────────────────────────────────\n",
    "question = \"What is the self-attention mechanism about?\"\n",
    "print(\"Q:\", question)\n",
    "print(\"A:\", answer_question(question, top_k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268d209",
   "metadata": {},
   "source": [
    "# Multi-turn Chat with KV-Cache\n",
    "\n",
    "To support interactive chat, we cache past key-value states of the LLM to avoid redundant computation.\n",
    "\n",
    "This reduces latency and enables more fluent multi-turn dialogues.  \n",
    "Here we test this by simulating a two-turn interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6a81e06-b4c0-489e-b1c1-068e0129f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: KV‐Cache FOR MULTI‐TURN SPEEDUP ───────────────────────────────────\n",
    "# (Only works if your Groq client supports returning a cache_id)\n",
    "def answer_with_cache(question: str,\n",
    "                      cache_id: str | None = None,\n",
    "                      top_k: int = 5) -> tuple[str, str]:\n",
    "    \"\"\"Returns (answer, new_cache_id).\"\"\"\n",
    "    # Retrieve as before\n",
    "    q_vecs = embedder.encode([question])\n",
    "    _, idxs = index.search(q_vecs.astype(\"float32\"), top_k)\n",
    "    ctx = [chunks[i] for i in idxs[0]]\n",
    "\n",
    "    # Build messages\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a factual assistant. Answer using ONLY the provided context.\"\n",
    "        )\n",
    "    }\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Context:\\n\"\n",
    "                   + \"\\n\".join(f\"- {c}\" for c in ctx)\n",
    "                   + f\"\\n\\nQuestion: {question}\"\n",
    "    }\n",
    "\n",
    "    # Send cache_id if available\n",
    "    params = {\n",
    "        \"model\": \"compound-beta-mini\",\n",
    "        \"messages\": [system_msg, user_msg],\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "    if cache_id:\n",
    "        params[\"cache_id\"] = cache_id\n",
    "\n",
    "    resp = client.chat.completions.create(**params)\n",
    "    new_cache = getattr(resp, \"cache_id\", None)\n",
    "    return resp.choices[0].message.content, new_cache\n",
    "\n",
    "# Example usage:\n",
    "# ans, cache = answer_with_cache(\"What is attention?\", None)\n",
    "# ans2, cache = answer_with_cache(\"How does it work?\", cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfb9d2",
   "metadata": {},
   "source": [
    "# Entity and Relation Extraction\n",
    "\n",
    "We extract named entities and their relationships from the PDF text to construct a basic knowledge graph.\n",
    "\n",
    "This graph enables structured queries and summarization, complementing the RAG architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e1d35f1-b40f-4c67-863c-970d43bbd1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch', 'PERSON'), ('usz@google.com', 'ORG'), ('GoogleResearch UniversityofToronto GoogleBrain', 'ORG'), ('aidan@cs.toronto.edu', 'PERSON'), ('Transformer', 'ORG'), ('two', 'CARDINAL'), ('28.4', 'CARDINAL'), ('WMT', 'ORG'), ('2014', 'DATE'), ('German', 'NORP'), ('longshort', 'GPE')], [('usz@google.com', 'llion@google.com', 'GoogleResearch UniversityofToronto GoogleBrain'), ('28.4', 'achieve', 'WMT')])\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 7: MINI KNOWLEDGE GRAPH (IE AGENT) ─────────────────────────────────\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class InformationExtractionAgent:\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def extract(self, text: str) -> tuple[list[tuple[str,str]], list[tuple[str,str,str]]]:\n",
    "        \"\"\"Returns (entities, relations).\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        rels = []\n",
    "        for sent in doc.sents:\n",
    "            sent_ents = sent.ents\n",
    "            if len(sent_ents) >= 2:\n",
    "                rels.append((\n",
    "                    sent_ents[0].text,\n",
    "                    sent.root.lemma_,\n",
    "                    sent_ents[1].text\n",
    "                ))\n",
    "        return ents, rels\n",
    "\n",
    "# Quick test:\n",
    "ie = InformationExtractionAgent()\n",
    "print(ie.extract(chunks[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e9d3d",
   "metadata": {},
   "source": [
    "# Dialogue History Support\n",
    "\n",
    "We enhance the prompt construction by incorporating prior conversation turns.  \n",
    "This allows the LLM to produce context-aware answers in multi-turn scenarios.\n",
    "\n",
    "We compare output with and without history for the same query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31c4f6ac-9928-408d-ad47-585dc3c1ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go ahead and ask your question about the text. I'll respond with a simple answer if it's a simple question, and take my time to reason if it requires more thought. I'm ready when you are.\n",
      "I've taken note of the provided context. I'm ready to help with your questions. Go ahead and ask away!\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 8: DIALOGUE HISTORY SUPPORT ────────────────────────────────────────\n",
    "from collections import deque\n",
    "\n",
    "# keep last N turns\n",
    "HISTORY_SIZE = 5\n",
    "history = deque(maxlen=HISTORY_SIZE * 2)  # store alternating (user, assistant)\n",
    "\n",
    "def answer_with_history(question: str, history: deque, top_k: int = 5) -> str:\n",
    "    # append the new user turn\n",
    "    history.append((\"user\", question))\n",
    "\n",
    "    # retrieve as before\n",
    "    q_vecs = embedder.encode([question])\n",
    "    _, idxs = index.search(q_vecs.astype(\"float32\"), top_k)\n",
    "    ctx = [chunks[i] for i in idxs[0]]\n",
    "\n",
    "    # build messages including history\n",
    "    msgs = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a factual, context-grounded assistant. \"\n",
    "            \"Use ONLY the provided context; do not hallucinate.\"\n",
    "        )\n",
    "    }]\n",
    "    # rehydrate history\n",
    "    for role, text in history:\n",
    "        msgs.append({\"role\": role, \"content\": text})\n",
    "\n",
    "    # add context + question as latest user message\n",
    "    context_block = \"Context:\\n\" + \"\\n\".join(f\"- {c}\" for c in ctx)\n",
    "    msgs.append({\"role\": \"user\", \"content\": context_block})\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"compound-beta-mini\",\n",
    "        messages=msgs,\n",
    "        max_tokens=256,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    answer = resp.choices[0].message.content\n",
    "    history.append((\"assistant\", answer))\n",
    "    return answer\n",
    "\n",
    "# Example:\n",
    "print(answer_with_history(\"First question?\", history))\n",
    "print(answer_with_history(\"Follow-up?\", history))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66afa70",
   "metadata": {},
   "source": [
    "# Agentic RAG Design\n",
    "\n",
    "We define three agents:\n",
    "- **Information Extraction Agent:** Identifies key facts and entities.\n",
    "- **Synthesis Agent:** Organizes and summarizes extracted knowledge.\n",
    "- **Query Agent:** Handles natural language queries using context or structured data.\n",
    "\n",
    "A simple JSON schema governs inter-agent communication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51897a3f-9842-48e9-8c2c-ebe63aaaea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:\n",
      "[('NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch', 'PERSON'), ('usz@google.com', 'ORG'), ('GoogleResearch UniversityofToronto GoogleBrain', 'ORG'), ('aidan@cs.toronto.edu', 'PERSON'), ('Transformer', 'ORG'), ('two', 'CARDINAL'), ('28.4', 'CARDINAL'), ('WMT', 'ORG'), ('2014', 'DATE'), ('German', 'NORP'), ('longshort', 'GPE'), ('Ashish', 'NORP'), ('withIllia', 'GPE'), ('LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand', 'ORG'), ('CA', 'GPE'), ('USA', 'GPE'), ('Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput', 'PERSON'), ('InthisworkweproposetheTransformer', 'ORG'), ('TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin', 'ORG'), ('2', 'CARDINAL'), ('ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU', 'ORG'), ('20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding', 'CARDINAL'), ('11', 'CARDINAL'), ('Transformer', 'ORG'), ('Transformer', 'ORG'), ('first', 'ORDINAL'), ('3', 'CARDINAL')]\n",
      "\n",
      "Relations:\n",
      "[('usz@google.com', 'llion@google.com', 'GoogleResearch UniversityofToronto GoogleBrain'), ('28.4', 'achieve', 'WMT'), ('Ashish', 'hasbeencruciallyinvolvedineveryaspectofthiswork', 'withIllia'), ('CA', '31stconferenceonneuralinformationprocessingsystems(nips2017),longbeach', 'USA'), ('2', 'computinghiddenrepresentationsinparallelforallinputandoutputposition', 'ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU'), ('Transformer', 'be', 'first')]\n",
      "Self-attention is a mechanism that allows a model to attend to all positions in the input sequence simultaneously and weigh their importance. It's a key component of the Transformer model, allowing it to handle long-range dependencies in sequences without recurrence or convolution. In the context of the Transformer, self-attention layers enable each position in the encoder or decoder to attend to all positions in the previous layer, allowing the model to capture complex relationships between distant elements in the sequence.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 9: AGENTIC ARCHITECTURE & COORDINATOR ───────────────────────────────\n",
    "import json\n",
    "\n",
    "class QueryAgent:\n",
    "    def __init__(self, ie_agent, rag_fn):\n",
    "        self.ie_agent = ie_agent\n",
    "        self.rag_fn   = rag_fn\n",
    "\n",
    "    def handle(self, user_input: str) -> str:\n",
    "        # If the user explicitly asks for entities/relations\n",
    "        if user_input.lower().startswith(\"tell me entities\"):\n",
    "            text = \"\\n\".join(chunks[:3])  # sample first few chunks\n",
    "            ents, rels = self.ie_agent.extract(text)\n",
    "            return f\"Entities:\\n{ents}\\n\\nRelations:\\n{rels}\"\n",
    "        # otherwise route to RAG\n",
    "        return self.rag_fn(user_input)\n",
    "\n",
    "class Coordinator:\n",
    "    def __init__(self, agents: dict[str, object]):\n",
    "        self.agents = agents\n",
    "\n",
    "    def dispatch(self, user_input: str) -> str:\n",
    "        # choose agent based on simple rules or JSON protocol\n",
    "        try:\n",
    "            # Here we could parse JSON commands, etc.\n",
    "            return self.agents[\"query\"].handle(user_input)\n",
    "        except Exception as e:\n",
    "            # fallback safe answer\n",
    "            return \"Sorry, I hit an error. Please rephrase.\"\n",
    "\n",
    "# wiring it together\n",
    "ie_agent   = InformationExtractionAgent()\n",
    "rag_agent  = lambda q: answer_with_history(q, history)\n",
    "query_agent = QueryAgent(ie_agent, rag_agent)\n",
    "coord = Coordinator({\"query\": query_agent})\n",
    "\n",
    "# Final test:\n",
    "print(coord.dispatch(\"Tell me entities from the paper.\"))\n",
    "print(coord.dispatch(\"What is self-attention?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab8b0f94-483d-4e94-a1f1-2b164e9d96e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 RAG Chat over the Attention Paper. Type ‘exit’ to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  wht is the architecture used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The Transformer model architecture. It uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder and decoder are composed of a stack of 6 identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  how is it better\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The Transformer model is better in several ways:\n",
      "\n",
      "1. **Parallelization**: The Transformer model allows for significantly more parallelization, which reduces the training time. This is because it relies entirely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "2. **Training Time**: The Transformer model requires significantly less time to train. For example, it can achieve a state-of-the-art BLEU score of 28.4 on the WMT2014 English-to-German translation task in 3.5 days on 8 P100 GPUs.\n",
      "\n",
      "3. **Translation Quality**: The Transformer model achieves better translation quality. On the WMT2014 English-to-German translation task, it outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU. On the WMT2014 English-to-French translation task, it establishes a new single-model state-of-the-art BLEU score of 41.0.\n",
      "\n",
      "4. **Computational Efficiency**: The Transformer model has a constant number of operations required to relate signals from two arbitrary input or output positions, which makes it more efficient than models that use convolutional neural networks or recurrent neural networks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 10: INTERACTIVE CHAT LOOP ──────────────────────────────────────────\n",
    "\n",
    "print(\"📖 RAG Chat over the Attention Paper. Type ‘exit’ to quit.\")\n",
    "cache_id = None  # for KV‐cache\n",
    "while True:\n",
    "    user_in = input(\"\\nYou: \")\n",
    "    if user_in.lower() in (\"exit\", \"quit\"):\n",
    "        break\n",
    "\n",
    "    # Example routing: if it contains “entities”, go IE; otherwise QA w/ history & cache\n",
    "    if user_in.lower().startswith(\"tell me entities\"):\n",
    "        out = ie_agent.extract(raw_text)    # run IE on full text or first chunks\n",
    "        print(\"\\nEntities:\\n\", out[0])\n",
    "        print(\"\\nRelations:\\n\", out[1])\n",
    "\n",
    "    else:\n",
    "        # 1) Use KV‐cache version (optional)\n",
    "        answer, cache_id = answer_with_cache(user_in, cache_id, top_k=5)\n",
    "\n",
    "        # 2) Then feed that answer into your history‐aware function\n",
    "        #    (if you prefer to combine both cache and history)\n",
    "        # answer = answer_with_history(user_in, history, top_k=5)\n",
    "\n",
    "        print(\"\\nAssistant:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b6ed1",
   "metadata": {},
   "source": [
    "# Synthesis Agent: Summary Generation\n",
    "\n",
    "This agent processes structured or semi-structured data (e.g., entities and relations) and returns a clean, coherent summary.\n",
    "\n",
    "It's useful for building reports or presenting answers from the knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "645b42d1-0884-4503-96bf-e642eb352b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " Here are 3 bullet points summarizing the text:\n",
      "\n",
      "* The authors propose a new neural network architecture called the Transformer, which relies entirely on attention mechanisms and eliminates the need for recurrent or convolutional layers.\n",
      "* The Transformer model achieves state-of-the-art results in machine translation tasks, with a BLEU score of 28.4 on the WMT2014 English-to-German translation task and 41.0 on the WMT2014 English-to-French translation task, while requiring significantly less training time and being more parallelizable.\n",
      "* Unlike traditional recurrent models, which process input sequences sequentially and have limited parallelization capabilities, the Transformer model uses self-attention mechanisms to draw global dependencies between input and output sequences, allowing for more efficient computation and improved performance.\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 11: SYNTHESIS AGENT ────────────────────────────────────────────────\n",
    "class SynthesisAgent:\n",
    "    def __init__(self, client, model=\"compound-beta-mini\"):\n",
    "        self.client = client\n",
    "        self.model  = model\n",
    "\n",
    "    def summarize(self, text: str, max_tokens=128) -> str:\n",
    "        prompt = (\n",
    "            \"Summarize the following text in 3 bullet points:\\n\\n\" +\n",
    "            text\n",
    "        )\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful summarizer.\"},\n",
    "                {\"role\": \"user\",   \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "# Quick test:\n",
    "synth = SynthesisAgent(client)\n",
    "print(\"Summary:\\n\", synth.summarize(\"\\n\".join(chunks[:3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42db34a",
   "metadata": {},
   "source": [
    "# Query Agent: Natural Language Question Answering\n",
    "\n",
    "The query agent understands user queries and either:\n",
    "- Retrieves relevant context via FAISS\n",
    "- Or queries the graph for structured answers\n",
    "\n",
    "It then forwards the result to the LLM for final response generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd378053-b26e-4e31-b891-4744d789afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 12: EXTENDED QUERY AGENT ────────────────────────────────────────\n",
    "class QueryAgent:\n",
    "    def __init__(self, ie_agent, synth_agent, rag_fn):\n",
    "        self.ie     = ie_agent\n",
    "        self.synth  = synth_agent\n",
    "        self.rag    = rag_fn\n",
    "\n",
    "    def handle(self, text: str) -> str:\n",
    "        t = text.lower()\n",
    "        if t.startswith(\"tell me entities\"):\n",
    "            ents, rels = self.ie.extract(raw_text)\n",
    "            return f\"Entities: {ents}\\nRelations: {rels}\"\n",
    "        if t.startswith(\"summarize\"):\n",
    "            # e.g. “summarize first 3 chunks”\n",
    "            return self.synth.summarize(\"\\n\".join(chunks[:3]))\n",
    "        # else default to RAG\n",
    "        return self.rag(text)\n",
    "\n",
    "# wire up\n",
    "synth_agent = SynthesisAgent(client)\n",
    "query_agent = QueryAgent(ie_agent, synth_agent, lambda q: answer_with_history(q, history))\n",
    "coord = Coordinator({\"query\": query_agent})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc189a0a-1b56-4f3c-95ea-d5f0abf4ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: [('NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch', 'PERSON'), ('usz@google.com', 'ORG'), ('GoogleResearch UniversityofToronto GoogleBrain', 'ORG'), ('aidan@cs.toronto.edu', 'PERSON'), ('Transformer', 'ORG'), ('two', 'CARDINAL'), ('28.4', 'CARDINAL'), ('WMT', 'ORG'), ('2014', 'DATE'), ('German', 'NORP'), ('longshort', 'GPE'), ('Ashish', 'NORP'), ('withIllia', 'GPE'), ('LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand', 'ORG'), ('CA', 'GPE'), ('USA', 'GPE'), ('Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput', 'PERSON'), ('InthisworkweproposetheTransformer', 'ORG'), ('TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin', 'ORG'), ('2', 'CARDINAL'), ('ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU', 'ORG'), ('20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding', 'CARDINAL'), ('11', 'CARDINAL'), ('Transformer', 'ORG'), ('Transformer', 'ORG'), ('first', 'ORDINAL'), ('3', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('TheTransformerfollowsthisoverallarchitectureusingstackedself', 'ORG'), ('3.1 EncoderandDecoderStacks', 'CARDINAL'), ('6', 'CARDINAL'), ('two', 'CARDINAL'), ('2', 'CARDINAL'), ('two', 'CARDINAL'), ('1', 'CARDINAL'), ('512', 'CARDINAL'), ('6identicallayers', 'CARDINAL'), ('3.2', 'CARDINAL'), ('ScaledDot', 'ORG'), ('Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2', 'PERSON'), ('3', 'CARDINAL'), ('ScaledDot', 'ORG'), ('Multi-HeadAttention', 'PRODUCT'), ('2', 'CARDINAL'), ('√', 'PERSON'), ('Inpractice', 'ORG'), ('intoamatrixQ.', 'PERSON'), ('Wecompute\\nthematrixofoutputsas:', 'FAC'), ('K', 'ORG'), ('V)=softmax', 'PERSON'), ('1', 'CARDINAL'), ('Whileforsmallvaluesofd', 'PERSON'), ('3', 'CARDINAL'), ('dk\\n3.2.2 Multi-HeadAttention\\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys', 'PERSON'), ('k k v\\nqueries', 'PERSON'), ('keysandvalueswethenperformtheattentionfunctioninparallel', 'GPE'), ('yieldingd -dimensional\\nv\\noutput', 'ORG'), ('4Toillustratewhythedotproductsgetlarge', 'CARDINAL'), ('cid:80)dk\\nq k', 'PERSON'), ('4', 'CARDINAL'), ('1', 'CARDINAL'), ('KWK', 'ORG'), ('∈Rdmodel×dk', 'PERSON'), ('∈Rdmodel×dk', 'PERSON'), ('∈Rdmodel×dv', 'PERSON'), ('8', 'CARDINAL'), ('3.2.3', 'CARDINAL'), ('ApplicationsofAttentioninourModel', 'ORG'), ('TheTransformerusesmulti', 'ORG'), ('• In\"encoder-decoderattention\"layers', 'PRODUCT'), ('• Theencodercontainsself-attentionlayers', 'PERSON'), ('3.3', 'CARDINAL'), ('FFN(x)=max(0,xW', 'PERSON'), ('2', 'CARDINAL'), ('1', 'CARDINAL'), ('two', 'CARDINAL'), ('1', 'CARDINAL'), ('512', 'CARDINAL'), ('2048', 'DATE'), ('3.4', 'CARDINAL'), ('EmbeddingsandSoftmax', 'ORG'), ('Similarlytoothersequencetransductionmodels', 'PERSON'), ('3.5', 'CARDINAL'), ('PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution', 'ORG'), ('5', 'CARDINAL'), ('LayerType ComplexityperLayer Sequential', 'ORG'), ('Operations', 'ORG'), ('Recurrent O(n·d2', 'FAC'), ('Self-Attention(restricted', 'GPE'), ('Thepositionalencodingshavethesamedimensiond', 'ORG'), ('Inthiswork', 'ORG'), ('cos(pos/100002i', 'GPE'), ('PE', 'ORG'), ('4', 'CARDINAL'), ('WhySelf-Attention', 'EVENT'), ('∈ Rd', 'PERSON'), ('1', 'CARDINAL'), ('Oneisthetotalcomputationalcomplexityperlayer', 'GPE'), ('Anotheristheamountofcomputationthatcan', 'PERSON'), ('suchasword', 'GPE'), ('31]andbyte', 'ORDINAL'), ('6', 'CARDINAL'), ('Asingleconvolutionallayerwithkernelwidthk', 'ORG'), ('6', 'CARDINAL'), ('Assidebenefit', 'ORG'), ('5', 'CARDINAL'), ('Thissectiondescribesthetrainingregimeforourmodels', 'ORG'), ('5.1', 'CARDINAL'), ('WMT', 'ORG'), ('2014', 'DATE'), ('English', 'LANGUAGE'), ('German', 'NORP'), ('about 4.5 million', 'CARDINAL'), ('Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-', 'ORG'), ('ForEnglish-French', 'NORP'), ('2014English', 'CARDINAL'), ('5.2', 'CARDINAL'), ('HardwareandSchedule', 'ORG'), ('Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs', 'PERSON'), ('3.5days', 'CARDINAL'), ('5.3', 'CARDINAL'), ('1 2', 'CARDINAL'), ('accordingtotheformula', 'PERSON'), ('min(step_num−0.5,step_num·warmup_steps−1.5', 'NORP'), ('3', 'CARDINAL'), ('5.4', 'CARDINAL'), ('ResidualDropout Weapplydropout[27]totheoutputofeachsub', 'PERSON'), ('0.1', 'CARDINAL'), ('7', 'CARDINAL'), ('English', 'LANGUAGE'), ('GermanandEnglish', 'NORP'), ('BLEU', 'ORG'), ('Model\\nEN-DE', 'PRODUCT'), ('23.75', 'CARDINAL'), ('39.2 1.0·1020', 'CARDINAL'), ('24.6', 'CARDINAL'), ('39.92', 'CARDINAL'), ('2.3·1019 1.4·1020', 'CARDINAL'), ('1.5·1020', 'CARDINAL'), ('26.03', 'CARDINAL'), ('40.56', 'CARDINAL'), ('1.2·1020', 'CARDINAL'), ('Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1020', 'WORK_OF_ART'), ('26.30', 'CARDINAL'), ('41.16', 'CARDINAL'), ('1.8·1020 1.1·1021', 'CARDINAL'), ('41.29', 'CARDINAL'), ('1.2·1021', 'CARDINAL'), ('Transformer(basemodel', 'ORG'), ('27.3', 'CARDINAL'), ('38.1', 'CARDINAL'), ('3.3·1018', 'CARDINAL'), ('28.4', 'CARDINAL'), ('41.0', 'CARDINAL'), ('2.3·1019', 'CARDINAL'), ('LabelSmoothing Duringtraining', 'WORK_OF_ART'), ('0.1[30', 'CARDINAL'), ('6', 'CARDINAL'), ('6.1', 'CARDINAL'), ('OntheWMT2014English', 'NORP'), ('Germantranslationtask', 'GPE'), ('BLEU', 'ORG'), ('OntheWMT2014English', 'NORP'), ('0.1,insteadof0.3', 'CARDINAL'), ('Forthebasemodels', 'ORG'), ('Weestimatethenumberoffloatingpointoperationsusedtotraina', 'GPE'), ('6.2', 'CARDINAL'), ('ToevaluatetheimportanceofdifferentcomponentsoftheTransformer', 'ORG'), ('measuringthechangeinperformanceonEnglish', 'NORP'), ('newstest2013.', 'ORG'), ('WepresenttheseresultsinTable3', 'PERSON'), ('Section 3.2.2', 'LAW'), ('8', 'CARDINAL'), ('newstest2013', 'LOC'), ('PPL BLEU', 'ORG'), ('N', 'GPE'), ('6 512', 'DATE'), ('64', 'CARDINAL'), ('65', 'CARDINAL'), ('1 512', 'CARDINAL'), ('512', 'CARDINAL'), ('24.9', 'CARDINAL'), ('4', 'CARDINAL'), ('128', 'CARDINAL'), ('128', 'CARDINAL'), ('5.00', 'CARDINAL'), ('25.5', 'CARDINAL'), ('16', 'CARDINAL'), ('32', 'MONEY'), ('32', 'DATE'), ('4.91 25.8', 'MONEY'), ('32', 'CARDINAL'), ('25.4', 'CARDINAL'), ('16', 'CARDINAL'), ('25.1 58', 'CARDINAL'), ('32', 'CARDINAL'), ('5.01', 'MONEY'), ('25.4', 'CARDINAL'), ('60', 'CARDINAL'), ('2 6.11', 'DATE'), ('36', 'CARDINAL'), ('4', 'CARDINAL'), ('50', 'CARDINAL'), ('8', 'CARDINAL'), ('80', 'CARDINAL'), ('256', 'CARDINAL'), ('32', 'CARDINAL'), ('32', 'DATE'), ('5.75', 'CARDINAL'), ('24.5', 'CARDINAL'), ('28', 'CARDINAL'), ('1024', 'DATE'), ('128', 'CARDINAL'), ('128', 'CARDINAL'), ('4.66', 'CARDINAL'), ('26.0 168', 'CARDINAL'), ('1024', 'DATE'), ('5.12', 'MONEY'), ('25.4', 'MONEY'), ('4096', 'DATE'), ('4.75', 'MONEY'), ('26.2 90', 'CARDINAL'), ('0.0', 'CARDINAL'), ('24.6', 'CARDINAL'), ('0.2 4.95 25.5', 'PERCENT'), ('0.0', 'CARDINAL'), ('25.3', 'CARDINAL'), ('0.2', 'CARDINAL'), ('5.47', 'MONEY'), ('25.7', 'CARDINAL'), ('4.92 25.7', 'CARDINAL'), ('4096', 'DATE'), ('16', 'CARDINAL'), ('300', 'CARDINAL'), ('26.4 213', 'CARDINAL'), ('7 Conclusion\\nInthiswork', 'ORG'), ('Transformer', 'ORG'), ('WMT', 'ORG'), ('2014', 'DATE'), ('English', 'LANGUAGE'), ('German', 'NORP'), ('WMT', 'ORG'), ('English', 'LANGUAGE'), ('9', 'CARDINAL'), ('1] JimmyLeiBa', 'CARDINAL'), ('andGeoffreyEHinton', 'GPE'), ('arXivpreprint\\narXiv:1607.06450,2016', 'PERSON'), ('2', 'CARDINAL'), ('KyunghyunCho', 'ORG'), ('CoRR', 'ORG'), ('abs/1409.0473,2014', 'ORG'), ('3', 'CARDINAL'), ('DennyBritz', 'ORG'), ('AnnaGoldie', 'ORG'), ('Minh-ThangLuong', 'ORG'), ('CoRR', 'ORG'), ('4', 'CARDINAL'), ('LiDong', 'GPE'), ('Longshort', 'GPE'), ('arXivpreprintarXiv:1601.06733,2016', 'ORG'), ('5', 'CARDINAL'), ('KyunghyunCho', 'ORG'), ('BartvanMerrienboer', 'ORG'), ('CaglarGulcehre', 'ORG'), ('FethiBougares', 'ORG'), ('HolgerSchwenk', 'ORG'), ('CoRR', 'ORG'), ('6', 'CARDINAL'), ('Francois Chollet', 'LOC'), ('7', 'CARDINAL'), ('JunyoungChung', 'GPE'), ('ÇaglarGülçehre', 'GPE'), ('KyunghyunCho', 'ORG'), ('CoRR', 'ORG'), ('8', 'CARDINAL'), ('DavidGrangier', 'ORG'), ('DenisYarats', 'ORG'), ('Convolu-', 'GPE'), ('9', 'CARDINAL'), ('Alex Graves', 'PERSON'), ('10', 'CARDINAL'), ('Xiangyu Zhang', 'PERSON'), ('Jian Sun', 'PERSON'), ('the IEEE Conference on Computer Vision', 'ORG'), ('Recognition', 'ORG'), ('pages770–778,2016', 'GPE'), ('11', 'CARDINAL'), ('SeppHochreiter', 'ORG'), ('YoshuaBengio', 'ORG'), ('PaoloFrasconi', 'ORG'), ('Gradientflowin', 'PERSON'), ('12', 'CARDINAL'), ('Sepp Hochreiter', 'ORG'), ('Jürgen Schmidhuber', 'PERSON'), ('9(8):1735–1780,1997', 'CARDINAL'), ('13', 'CARDINAL'), ('OriolVinyals', 'ORG'), ('MikeSchuster', 'ORG'), ('NoamShazeer', 'ORG'), ('14', 'CARDINAL'), ('onLearningRepresentations(ICLR),2016', 'ORG'), ('15', 'CARDINAL'), ('NalKalchbrenner', 'ORG'), ('LasseEspeholt', 'ORG'), ('KarenSimonyan', 'ORG'), ('AaronvandenOord', 'ORG'), ('AlexGraves', 'ORG'), ('rayKavukcuoglu', 'ORG'), ('Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2', 'PERSON'), ('2017', 'DATE'), ('16', 'CARDINAL'), ('YoonKim', 'ORG'), ('CarlDenton', 'ORG'), ('LuongHoang', 'ORG'), ('17] DiederikKingmaandJimmyBa', 'CARDINAL'), ('Adam', 'PERSON'), ('18', 'CARDINAL'), ('arXivpreprint\\n', 'PERSON'), ('19', 'CARDINAL'), ('Zhouhan Lin', 'PERSON'), ('Minwei Feng', 'PERSON'), ('Cicero Nogueira', 'ORG'), ('Santos', 'GPE'), ('Mo Yu', 'PERSON'), ('Bing Xiang', 'PERSON'), ('Bowen\\nZhou', 'PERSON'), ('Yoshua Bengio', 'PERSON'), ('20', 'CARDINAL'), ('InformationProcessingSystems,(NIPS),2016', 'PERSON'), ('10', 'CARDINAL'), ('21', 'CARDINAL'), ('HieuPham', 'ORG'), ('22', 'CARDINAL'), ('AnkurParikh', 'ORG'), ('OscarTäckström', 'ORG'), ('DipanjanDas', 'ORG'), ('andJakobUszkoreit', 'GPE'), ('23', 'CARDINAL'), ('CaimingXiong', 'ORG'), ('24', 'CARDINAL'), ('preprintarXiv:1608.05859,2016', 'ORG'), ('25', 'CARDINAL'), ('RicoSennrich', 'ORG'), ('BarryHaddow', 'ORG'), ('26', 'CARDINAL'), ('NoamShazeer', 'ORG'), ('AzaliaMirhoseini', 'ORG'), ('KrzysztofMaziarz', 'ORG'), ('AndyDavis', 'ORG'), ('GeoffreyHinton', 'GPE'), ('27', 'CARDINAL'), ('NitishSrivastava', 'ORG'), ('AlexKrizhevsky', 'ORG'), ('IlyaSutskever', 'ORG'), ('JournalofMachine', 'ORG'), ('LearningResearch,15(1):1929–1958,2014', 'ORG'), ('28', 'CARDINAL'), ('Sainbayar Sukhbaatar', 'FAC'), ('arthur szlam', 'PERSON'), ('Jason Weston', 'PERSON'), ('Rob Fergus', 'PERSON'), ('N.D.Lawrence', 'GPE'), ('D.D.Lee', 'GPE'), ('M.Sugiyama', 'GPE'), ('AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates', 'ORG'), ('29', 'CARDINAL'), ('OriolVinyals', 'ORG'), ('30', 'CARDINAL'), ('ChristianSzegedy', 'ORG'), ('VincentVanhoucke', 'ORG'), ('SergeyIoffe', 'ORG'), ('JonathonShlens', 'PERSON'), ('CoRR', 'ORG'), ('31', 'CARDINAL'), ('Yonghui Wu', 'PERSON'), ('Mike Schuster', 'PERSON'), ('Zhifeng Chen', 'PERSON'), ('Quoc V Le', 'PERSON'), ('Mohammad Norouzi', 'PERSON'), ('Wolfgang', 'PERSON'), ('Macherey', 'GPE'), ('MaximKrikun', 'ORG'), ('YuanCao', 'ORG'), ('QinGao', 'ORG'), ('KlausMacherey', 'ORG'), ('arXivpreprint\\narXiv:1609.08144,2016', 'PERSON'), ('32', 'CARDINAL'), ('Jie Zhou', 'PERSON'), ('Ying Cao', 'PERSON'), ('Xuguang Wang', 'PERSON'), ('Peng Li', 'PERSON'), ('Wei Xu', 'PERSON'), ('CoRR', 'ORG'), ('abs/1606.04199,2016', 'NORP'), ('11', 'CARDINAL')]\n",
      "Relations: [('usz@google.com', 'llion@google.com', 'GoogleResearch UniversityofToronto GoogleBrain'), ('28.4', 'achieve', 'WMT'), ('Ashish', 'hasbeencruciallyinvolvedineveryaspectofthiswork', 'withIllia'), ('CA', '31stconferenceonneuralinformationprocessingsystems(nips2017),longbeach', 'USA'), ('2', 'computinghiddenrepresentationsinparallelforallinputandoutputposition', 'ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU'), ('Transformer', 'be', 'first'), ('3.1 EncoderandDecoderStacks', 'compose', '6'), ('two', 'weemployaresidualconnection[10]aroundeachof', '1'), ('ScaledDot', '3.2.1', 'Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2'), ('3', 'Figure', 'ScaledDot'), ('Inpractice', 'Inpractice', 'intoamatrixQ.'), ('k k v\\nqueries', 'Oneachoftheseprojectedversionsof', 'keysandvalueswethenperformtheattentionfunctioninparallel'), ('4Toillustratewhythedotproductsgetlarge', '4toillustratewhythedotproductsgetlarge', 'cid:80)dk\\nq k'), ('4', 'I', '1'), ('∈Rdmodel×dk', '∈Rdmodel×dk', '∈Rdmodel×dk'), ('3.2.3', 'andthememorykeysandvaluescomefromtheoutputoftheencoder', 'ApplicationsofAttentioninourModel'), ('FFN(x)=max(0,xW', 'consistsoftwolineartransformationswithareluactivationinbetween', '2'), ('two', 'be', '1'), ('512', 'be', '2048'), ('3.4', 'Similarlytoothersequencetransductionmodels', 'EmbeddingsandSoftmax'), ('3.5', 'model', 'PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution'), ('LayerType ComplexityperLayer Sequential', 'Operations', 'Operations'), ('4', 'compare', 'WhySelf-Attention'), ('suchasword', 'be', '31]andbyte'), ('5', 'Thissectiondescribesthetrainingregimeforourmodels', 'Thissectiondescribesthetrainingregimeforourmodels'), ('5.1', 'train', 'WMT'), ('ForEnglish-French', 'French', '2014English'), ('5.2', 'wetrainedourmodelsononemachinewith8nvidiap100gpu', 'HardwareandSchedule'), ('1 2', 'lrate', 'accordingtotheformula'), ('5.4', 'warmup_steps=4000', 'ResidualDropout Weapplydropout[27]totheoutputofeachsub'), ('0.1', 'drop', '7'), ('6', 'intable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0', '6.1'), ('6.2', 'ToevaluatetheimportanceofdifferentcomponentsoftheTransformer', 'ToevaluatetheimportanceofdifferentcomponentsoftheTransformer'), ('PPL BLEU', 'param', 'N'), ('6 512', 'drop', '64'), ('WMT', 'on', '2014'), ('9', 'References', '1] JimmyLeiBa'), ('2', 'DzmitryBahdanau', 'KyunghyunCho'), ('CoRR', 'CoRR', 'abs/1409.0473,2014'), ('3', 'DennyBritz', 'DennyBritz'), ('4', 'JianpengCheng', 'LiDong'), ('5', 'KyunghyunCho', 'KyunghyunCho'), ('6', 'Chollet', 'Francois Chollet'), ('7', 'JunyoungChung', 'JunyoungChung'), ('8', 'JonasGehring', 'DavidGrangier'), ('9', 'Graves', 'Alex Graves'), ('10', '10', 'Xiangyu Zhang'), ('the IEEE Conference on Computer Vision', 'in', 'Recognition'), ('11', 'SeppHochreiter', 'SeppHochreiter'), ('12', 'Hochreiter', 'Sepp Hochreiter'), ('13', 'RafalJozefowicz', 'OriolVinyals'), ('15', 'NalKalchbrenner', 'NalKalchbrenner'), ('16', 'YoonKim', 'YoonKim'), ('19', 'Lin', 'Zhouhan Lin'), ('21', 'basedneuralmachinetranslation', 'HieuPham'), ('22', 'AnkurParikh', 'AnkurParikh'), ('23', 'RomainPaulus', 'CaimingXiong'), ('25', 'RicoSennrich', 'RicoSennrich'), ('26', 'NoamShazeer', 'NoamShazeer'), ('27', 'Dropout', 'NitishSrivastava'), ('JournalofMachine', 'learningresearch,15(1):1929–1958,2014', 'LearningResearch,15(1):1929–1958,2014'), ('28', 'Sukhbaatar', 'Sainbayar Sukhbaatar'), ('N.D.Lawrence', 'inc.corte', 'D.D.Lee'), ('29', 'IlyaSutskever', 'OriolVinyals'), ('30', 'ChristianSzegedy', 'ChristianSzegedy'), ('31', 'Wu', 'Yonghui Wu'), ('32', 'Zhou', 'Jie Zhou'), ('CoRR', 'CoRR', 'abs/1606.04199,2016')]\n",
      "Here are 3 bullet points summarizing the text:\n",
      "\n",
      "* The authors introduce the Transformer, a new neural network architecture that relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional layers.\n",
      "* The Transformer model achieves state-of-the-art results in machine translation tasks, with a BLEU score of 28.4 on English-to-German and 41.0 on English-to-French, while requiring significantly less training time and being more parallelizable.\n",
      "* The Transformer architecture is designed to address the limitations of traditional recurrent models, which are inherently sequential and difficult to parallelize, and convolutional models, which struggle to learn dependencies between distant positions; the Transformer uses self-attention mechanisms to compute representations of input and output sequences in parallel.\n",
      "You've already asked this question. Self-attention is a mechanism that allows a model to attend to all positions in the input sequence simultaneously and weigh their importance. It's a key component of the Transformer model, allowing it to handle long-range dependencies in sequences without recurrence or convolution. In the context of the Transformer, self-attention layers enable each position in the encoder or decoder to attend to all positions in the previous layer, allowing the model to capture complex relationships between distant elements in the sequence.\n"
     ]
    }
   ],
   "source": [
    "# After running Cell 12, in a new cell:\n",
    "print(coord.dispatch(\"Tell me entities from the paper.\"))\n",
    "print(coord.dispatch(\"Summarize the first section\"))\n",
    "print(coord.dispatch(\"What is self-attention?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2f083",
   "metadata": {},
   "source": [
    "# Conclusion and Observations\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) system over a PDF using:\n",
    "- FAISS for fast semantic retrieval\n",
    "- GROQ API for open-source LLM inference\n",
    "- Optional extensions like KV-cache, graph extraction, and agent orchestration\n",
    "\n",
    "**Results:**\n",
    "- Chunk retrievals were mostly relevant (top-3 accuracy acceptable).\n",
    "- GROQ's performance was good with small prompt windows; history improved coherence.\n",
    "- Agentic architecture offers modularity but adds complexity.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8e2e854-42ac-4205-967a-d80c2a7f4040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 RAG+Agents over Attention Paper. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Tell me entities and relations from the paper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Entities: [('NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch', 'PERSON'), ('usz@google.com', 'ORG'), ('GoogleResearch UniversityofToronto GoogleBrain', 'ORG'), ('aidan@cs.toronto.edu', 'PERSON'), ('Transformer', 'ORG'), ('two', 'CARDINAL'), ('28.4', 'CARDINAL'), ('WMT', 'ORG'), ('2014', 'DATE'), ('German', 'NORP'), ('longshort', 'GPE'), ('Ashish', 'NORP'), ('withIllia', 'GPE'), ('LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand', 'ORG'), ('CA', 'GPE'), ('USA', 'GPE'), ('Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput', 'PERSON'), ('InthisworkweproposetheTransformer', 'ORG'), ('TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin', 'ORG'), ('2', 'CARDINAL'), ('ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU', 'ORG'), ('20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding', 'CARDINAL'), ('11', 'CARDINAL'), ('Transformer', 'ORG'), ('Transformer', 'ORG'), ('first', 'ORDINAL'), ('3', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('1', 'CARDINAL'), ('TheTransformerfollowsthisoverallarchitectureusingstackedself', 'ORG'), ('3.1 EncoderandDecoderStacks', 'CARDINAL'), ('6', 'CARDINAL'), ('two', 'CARDINAL'), ('2', 'CARDINAL'), ('two', 'CARDINAL'), ('1', 'CARDINAL'), ('512', 'CARDINAL'), ('6identicallayers', 'CARDINAL'), ('3.2', 'CARDINAL'), ('ScaledDot', 'ORG'), ('Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2', 'PERSON'), ('3', 'CARDINAL'), ('ScaledDot', 'ORG'), ('Multi-HeadAttention', 'PRODUCT'), ('2', 'CARDINAL'), ('√', 'PERSON'), ('Inpractice', 'ORG'), ('intoamatrixQ.', 'PERSON'), ('Wecompute\\nthematrixofoutputsas:', 'FAC'), ('K', 'ORG'), ('V)=softmax', 'PERSON'), ('1', 'CARDINAL'), ('Whileforsmallvaluesofd', 'PERSON'), ('3', 'CARDINAL'), ('dk\\n3.2.2 Multi-HeadAttention\\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys', 'PERSON'), ('k k v\\nqueries', 'PERSON'), ('keysandvalueswethenperformtheattentionfunctioninparallel', 'GPE'), ('yieldingd -dimensional\\nv\\noutput', 'ORG'), ('4Toillustratewhythedotproductsgetlarge', 'CARDINAL'), ('cid:80)dk\\nq k', 'PERSON'), ('4', 'CARDINAL'), ('1', 'CARDINAL'), ('KWK', 'ORG'), ('∈Rdmodel×dk', 'PERSON'), ('∈Rdmodel×dk', 'PERSON'), ('∈Rdmodel×dv', 'PERSON'), ('8', 'CARDINAL'), ('3.2.3', 'CARDINAL'), ('ApplicationsofAttentioninourModel', 'ORG'), ('TheTransformerusesmulti', 'ORG'), ('• In\"encoder-decoderattention\"layers', 'PRODUCT'), ('• Theencodercontainsself-attentionlayers', 'PERSON'), ('3.3', 'CARDINAL'), ('FFN(x)=max(0,xW', 'PERSON'), ('2', 'CARDINAL'), ('1', 'CARDINAL'), ('two', 'CARDINAL'), ('1', 'CARDINAL'), ('512', 'CARDINAL'), ('2048', 'DATE'), ('3.4', 'CARDINAL'), ('EmbeddingsandSoftmax', 'ORG'), ('Similarlytoothersequencetransductionmodels', 'PERSON'), ('3.5', 'CARDINAL'), ('PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution', 'ORG'), ('5', 'CARDINAL'), ('LayerType ComplexityperLayer Sequential', 'ORG'), ('Operations', 'ORG'), ('Recurrent O(n·d2', 'FAC'), ('Self-Attention(restricted', 'GPE'), ('Thepositionalencodingshavethesamedimensiond', 'ORG'), ('Inthiswork', 'ORG'), ('cos(pos/100002i', 'GPE'), ('PE', 'ORG'), ('4', 'CARDINAL'), ('WhySelf-Attention', 'EVENT'), ('∈ Rd', 'PERSON'), ('1', 'CARDINAL'), ('Oneisthetotalcomputationalcomplexityperlayer', 'GPE'), ('Anotheristheamountofcomputationthatcan', 'PERSON'), ('suchasword', 'GPE'), ('31]andbyte', 'ORDINAL'), ('6', 'CARDINAL'), ('Asingleconvolutionallayerwithkernelwidthk', 'ORG'), ('6', 'CARDINAL'), ('Assidebenefit', 'ORG'), ('5', 'CARDINAL'), ('Thissectiondescribesthetrainingregimeforourmodels', 'ORG'), ('5.1', 'CARDINAL'), ('WMT', 'ORG'), ('2014', 'DATE'), ('English', 'LANGUAGE'), ('German', 'NORP'), ('about 4.5 million', 'CARDINAL'), ('Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-', 'ORG'), ('ForEnglish-French', 'NORP'), ('2014English', 'CARDINAL'), ('5.2', 'CARDINAL'), ('HardwareandSchedule', 'ORG'), ('Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs', 'PERSON'), ('3.5days', 'CARDINAL'), ('5.3', 'CARDINAL'), ('1 2', 'CARDINAL'), ('accordingtotheformula', 'PERSON'), ('min(step_num−0.5,step_num·warmup_steps−1.5', 'NORP'), ('3', 'CARDINAL'), ('5.4', 'CARDINAL'), ('ResidualDropout Weapplydropout[27]totheoutputofeachsub', 'PERSON'), ('0.1', 'CARDINAL'), ('7', 'CARDINAL'), ('English', 'LANGUAGE'), ('GermanandEnglish', 'NORP'), ('BLEU', 'ORG'), ('Model\\nEN-DE', 'PRODUCT'), ('23.75', 'CARDINAL'), ('39.2 1.0·1020', 'CARDINAL'), ('24.6', 'CARDINAL'), ('39.92', 'CARDINAL'), ('2.3·1019 1.4·1020', 'CARDINAL'), ('1.5·1020', 'CARDINAL'), ('26.03', 'CARDINAL'), ('40.56', 'CARDINAL'), ('1.2·1020', 'CARDINAL'), ('Deep-Att+PosUnkEnsemble[32] 40.4 8.0·1020', 'WORK_OF_ART'), ('26.30', 'CARDINAL'), ('41.16', 'CARDINAL'), ('1.8·1020 1.1·1021', 'CARDINAL'), ('41.29', 'CARDINAL'), ('1.2·1021', 'CARDINAL'), ('Transformer(basemodel', 'ORG'), ('27.3', 'CARDINAL'), ('38.1', 'CARDINAL'), ('3.3·1018', 'CARDINAL'), ('28.4', 'CARDINAL'), ('41.0', 'CARDINAL'), ('2.3·1019', 'CARDINAL'), ('LabelSmoothing Duringtraining', 'WORK_OF_ART'), ('0.1[30', 'CARDINAL'), ('6', 'CARDINAL'), ('6.1', 'CARDINAL'), ('OntheWMT2014English', 'NORP'), ('Germantranslationtask', 'GPE'), ('BLEU', 'ORG'), ('OntheWMT2014English', 'NORP'), ('0.1,insteadof0.3', 'CARDINAL'), ('Forthebasemodels', 'ORG'), ('Weestimatethenumberoffloatingpointoperationsusedtotraina', 'GPE'), ('6.2', 'CARDINAL'), ('ToevaluatetheimportanceofdifferentcomponentsoftheTransformer', 'ORG'), ('measuringthechangeinperformanceonEnglish', 'NORP'), ('newstest2013.', 'ORG'), ('WepresenttheseresultsinTable3', 'PERSON'), ('Section 3.2.2', 'LAW'), ('8', 'CARDINAL'), ('newstest2013', 'LOC'), ('PPL BLEU', 'ORG'), ('N', 'GPE'), ('6 512', 'DATE'), ('64', 'CARDINAL'), ('65', 'CARDINAL'), ('1 512', 'CARDINAL'), ('512', 'CARDINAL'), ('24.9', 'CARDINAL'), ('4', 'CARDINAL'), ('128', 'CARDINAL'), ('128', 'CARDINAL'), ('5.00', 'CARDINAL'), ('25.5', 'CARDINAL'), ('16', 'CARDINAL'), ('32', 'MONEY'), ('32', 'DATE'), ('4.91 25.8', 'MONEY'), ('32', 'CARDINAL'), ('25.4', 'CARDINAL'), ('16', 'CARDINAL'), ('25.1 58', 'CARDINAL'), ('32', 'CARDINAL'), ('5.01', 'MONEY'), ('25.4', 'CARDINAL'), ('60', 'CARDINAL'), ('2 6.11', 'DATE'), ('36', 'CARDINAL'), ('4', 'CARDINAL'), ('50', 'CARDINAL'), ('8', 'CARDINAL'), ('80', 'CARDINAL'), ('256', 'CARDINAL'), ('32', 'CARDINAL'), ('32', 'DATE'), ('5.75', 'CARDINAL'), ('24.5', 'CARDINAL'), ('28', 'CARDINAL'), ('1024', 'DATE'), ('128', 'CARDINAL'), ('128', 'CARDINAL'), ('4.66', 'CARDINAL'), ('26.0 168', 'CARDINAL'), ('1024', 'DATE'), ('5.12', 'MONEY'), ('25.4', 'MONEY'), ('4096', 'DATE'), ('4.75', 'MONEY'), ('26.2 90', 'CARDINAL'), ('0.0', 'CARDINAL'), ('24.6', 'CARDINAL'), ('0.2 4.95 25.5', 'PERCENT'), ('0.0', 'CARDINAL'), ('25.3', 'CARDINAL'), ('0.2', 'CARDINAL'), ('5.47', 'MONEY'), ('25.7', 'CARDINAL'), ('4.92 25.7', 'CARDINAL'), ('4096', 'DATE'), ('16', 'CARDINAL'), ('300', 'CARDINAL'), ('26.4 213', 'CARDINAL'), ('7 Conclusion\\nInthiswork', 'ORG'), ('Transformer', 'ORG'), ('WMT', 'ORG'), ('2014', 'DATE'), ('English', 'LANGUAGE'), ('German', 'NORP'), ('WMT', 'ORG'), ('English', 'LANGUAGE'), ('9', 'CARDINAL'), ('1] JimmyLeiBa', 'CARDINAL'), ('andGeoffreyEHinton', 'GPE'), ('arXivpreprint\\narXiv:1607.06450,2016', 'PERSON'), ('2', 'CARDINAL'), ('KyunghyunCho', 'ORG'), ('CoRR', 'ORG'), ('abs/1409.0473,2014', 'ORG'), ('3', 'CARDINAL'), ('DennyBritz', 'ORG'), ('AnnaGoldie', 'ORG'), ('Minh-ThangLuong', 'ORG'), ('CoRR', 'ORG'), ('4', 'CARDINAL'), ('LiDong', 'GPE'), ('Longshort', 'GPE'), ('arXivpreprintarXiv:1601.06733,2016', 'ORG'), ('5', 'CARDINAL'), ('KyunghyunCho', 'ORG'), ('BartvanMerrienboer', 'ORG'), ('CaglarGulcehre', 'ORG'), ('FethiBougares', 'ORG'), ('HolgerSchwenk', 'ORG'), ('CoRR', 'ORG'), ('6', 'CARDINAL'), ('Francois Chollet', 'LOC'), ('7', 'CARDINAL'), ('JunyoungChung', 'GPE'), ('ÇaglarGülçehre', 'GPE'), ('KyunghyunCho', 'ORG'), ('CoRR', 'ORG'), ('8', 'CARDINAL'), ('DavidGrangier', 'ORG'), ('DenisYarats', 'ORG'), ('Convolu-', 'GPE'), ('9', 'CARDINAL'), ('Alex Graves', 'PERSON'), ('10', 'CARDINAL'), ('Xiangyu Zhang', 'PERSON'), ('Jian Sun', 'PERSON'), ('the IEEE Conference on Computer Vision', 'ORG'), ('Recognition', 'ORG'), ('pages770–778,2016', 'GPE'), ('11', 'CARDINAL'), ('SeppHochreiter', 'ORG'), ('YoshuaBengio', 'ORG'), ('PaoloFrasconi', 'ORG'), ('Gradientflowin', 'PERSON'), ('12', 'CARDINAL'), ('Sepp Hochreiter', 'ORG'), ('Jürgen Schmidhuber', 'PERSON'), ('9(8):1735–1780,1997', 'CARDINAL'), ('13', 'CARDINAL'), ('OriolVinyals', 'ORG'), ('MikeSchuster', 'ORG'), ('NoamShazeer', 'ORG'), ('14', 'CARDINAL'), ('onLearningRepresentations(ICLR),2016', 'ORG'), ('15', 'CARDINAL'), ('NalKalchbrenner', 'ORG'), ('LasseEspeholt', 'ORG'), ('KarenSimonyan', 'ORG'), ('AaronvandenOord', 'ORG'), ('AlexGraves', 'ORG'), ('rayKavukcuoglu', 'ORG'), ('Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2', 'PERSON'), ('2017', 'DATE'), ('16', 'CARDINAL'), ('YoonKim', 'ORG'), ('CarlDenton', 'ORG'), ('LuongHoang', 'ORG'), ('17] DiederikKingmaandJimmyBa', 'CARDINAL'), ('Adam', 'PERSON'), ('18', 'CARDINAL'), ('arXivpreprint\\n', 'PERSON'), ('19', 'CARDINAL'), ('Zhouhan Lin', 'PERSON'), ('Minwei Feng', 'PERSON'), ('Cicero Nogueira', 'ORG'), ('Santos', 'GPE'), ('Mo Yu', 'PERSON'), ('Bing Xiang', 'PERSON'), ('Bowen\\nZhou', 'PERSON'), ('Yoshua Bengio', 'PERSON'), ('20', 'CARDINAL'), ('InformationProcessingSystems,(NIPS),2016', 'PERSON'), ('10', 'CARDINAL'), ('21', 'CARDINAL'), ('HieuPham', 'ORG'), ('22', 'CARDINAL'), ('AnkurParikh', 'ORG'), ('OscarTäckström', 'ORG'), ('DipanjanDas', 'ORG'), ('andJakobUszkoreit', 'GPE'), ('23', 'CARDINAL'), ('CaimingXiong', 'ORG'), ('24', 'CARDINAL'), ('preprintarXiv:1608.05859,2016', 'ORG'), ('25', 'CARDINAL'), ('RicoSennrich', 'ORG'), ('BarryHaddow', 'ORG'), ('26', 'CARDINAL'), ('NoamShazeer', 'ORG'), ('AzaliaMirhoseini', 'ORG'), ('KrzysztofMaziarz', 'ORG'), ('AndyDavis', 'ORG'), ('GeoffreyHinton', 'GPE'), ('27', 'CARDINAL'), ('NitishSrivastava', 'ORG'), ('AlexKrizhevsky', 'ORG'), ('IlyaSutskever', 'ORG'), ('JournalofMachine', 'ORG'), ('LearningResearch,15(1):1929–1958,2014', 'ORG'), ('28', 'CARDINAL'), ('Sainbayar Sukhbaatar', 'FAC'), ('arthur szlam', 'PERSON'), ('Jason Weston', 'PERSON'), ('Rob Fergus', 'PERSON'), ('N.D.Lawrence', 'GPE'), ('D.D.Lee', 'GPE'), ('M.Sugiyama', 'GPE'), ('AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates', 'ORG'), ('29', 'CARDINAL'), ('OriolVinyals', 'ORG'), ('30', 'CARDINAL'), ('ChristianSzegedy', 'ORG'), ('VincentVanhoucke', 'ORG'), ('SergeyIoffe', 'ORG'), ('JonathonShlens', 'PERSON'), ('CoRR', 'ORG'), ('31', 'CARDINAL'), ('Yonghui Wu', 'PERSON'), ('Mike Schuster', 'PERSON'), ('Zhifeng Chen', 'PERSON'), ('Quoc V Le', 'PERSON'), ('Mohammad Norouzi', 'PERSON'), ('Wolfgang', 'PERSON'), ('Macherey', 'GPE'), ('MaximKrikun', 'ORG'), ('YuanCao', 'ORG'), ('QinGao', 'ORG'), ('KlausMacherey', 'ORG'), ('arXivpreprint\\narXiv:1609.08144,2016', 'PERSON'), ('32', 'CARDINAL'), ('Jie Zhou', 'PERSON'), ('Ying Cao', 'PERSON'), ('Xuguang Wang', 'PERSON'), ('Peng Li', 'PERSON'), ('Wei Xu', 'PERSON'), ('CoRR', 'ORG'), ('abs/1606.04199,2016', 'NORP'), ('11', 'CARDINAL')]\n",
      "Relations: [('usz@google.com', 'llion@google.com', 'GoogleResearch UniversityofToronto GoogleBrain'), ('28.4', 'achieve', 'WMT'), ('Ashish', 'hasbeencruciallyinvolvedineveryaspectofthiswork', 'withIllia'), ('CA', '31stconferenceonneuralinformationprocessingsystems(nips2017),longbeach', 'USA'), ('2', 'computinghiddenrepresentationsinparallelforallinputandoutputposition', 'ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU'), ('Transformer', 'be', 'first'), ('3.1 EncoderandDecoderStacks', 'compose', '6'), ('two', 'weemployaresidualconnection[10]aroundeachof', '1'), ('ScaledDot', '3.2.1', 'Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2'), ('3', 'Figure', 'ScaledDot'), ('Inpractice', 'Inpractice', 'intoamatrixQ.'), ('k k v\\nqueries', 'Oneachoftheseprojectedversionsof', 'keysandvalueswethenperformtheattentionfunctioninparallel'), ('4Toillustratewhythedotproductsgetlarge', '4toillustratewhythedotproductsgetlarge', 'cid:80)dk\\nq k'), ('4', 'I', '1'), ('∈Rdmodel×dk', '∈Rdmodel×dk', '∈Rdmodel×dk'), ('3.2.3', 'andthememorykeysandvaluescomefromtheoutputoftheencoder', 'ApplicationsofAttentioninourModel'), ('FFN(x)=max(0,xW', 'consistsoftwolineartransformationswithareluactivationinbetween', '2'), ('two', 'be', '1'), ('512', 'be', '2048'), ('3.4', 'Similarlytoothersequencetransductionmodels', 'EmbeddingsandSoftmax'), ('3.5', 'model', 'PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution'), ('LayerType ComplexityperLayer Sequential', 'Operations', 'Operations'), ('4', 'compare', 'WhySelf-Attention'), ('suchasword', 'be', '31]andbyte'), ('5', 'Thissectiondescribesthetrainingregimeforourmodels', 'Thissectiondescribesthetrainingregimeforourmodels'), ('5.1', 'train', 'WMT'), ('ForEnglish-French', 'French', '2014English'), ('5.2', 'wetrainedourmodelsononemachinewith8nvidiap100gpu', 'HardwareandSchedule'), ('1 2', 'lrate', 'accordingtotheformula'), ('5.4', 'warmup_steps=4000', 'ResidualDropout Weapplydropout[27]totheoutputofeachsub'), ('0.1', 'drop', '7'), ('6', 'intable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0', '6.1'), ('6.2', 'ToevaluatetheimportanceofdifferentcomponentsoftheTransformer', 'ToevaluatetheimportanceofdifferentcomponentsoftheTransformer'), ('PPL BLEU', 'param', 'N'), ('6 512', 'drop', '64'), ('WMT', 'on', '2014'), ('9', 'References', '1] JimmyLeiBa'), ('2', 'DzmitryBahdanau', 'KyunghyunCho'), ('CoRR', 'CoRR', 'abs/1409.0473,2014'), ('3', 'DennyBritz', 'DennyBritz'), ('4', 'JianpengCheng', 'LiDong'), ('5', 'KyunghyunCho', 'KyunghyunCho'), ('6', 'Chollet', 'Francois Chollet'), ('7', 'JunyoungChung', 'JunyoungChung'), ('8', 'JonasGehring', 'DavidGrangier'), ('9', 'Graves', 'Alex Graves'), ('10', '10', 'Xiangyu Zhang'), ('the IEEE Conference on Computer Vision', 'in', 'Recognition'), ('11', 'SeppHochreiter', 'SeppHochreiter'), ('12', 'Hochreiter', 'Sepp Hochreiter'), ('13', 'RafalJozefowicz', 'OriolVinyals'), ('15', 'NalKalchbrenner', 'NalKalchbrenner'), ('16', 'YoonKim', 'YoonKim'), ('19', 'Lin', 'Zhouhan Lin'), ('21', 'basedneuralmachinetranslation', 'HieuPham'), ('22', 'AnkurParikh', 'AnkurParikh'), ('23', 'RomainPaulus', 'CaimingXiong'), ('25', 'RicoSennrich', 'RicoSennrich'), ('26', 'NoamShazeer', 'NoamShazeer'), ('27', 'Dropout', 'NitishSrivastava'), ('JournalofMachine', 'learningresearch,15(1):1929–1958,2014', 'LearningResearch,15(1):1929–1958,2014'), ('28', 'Sukhbaatar', 'Sainbayar Sukhbaatar'), ('N.D.Lawrence', 'inc.corte', 'D.D.Lee'), ('29', 'IlyaSutskever', 'OriolVinyals'), ('30', 'ChristianSzegedy', 'ChristianSzegedy'), ('31', 'Wu', 'Yonghui Wu'), ('32', 'Zhou', 'Jie Zhou'), ('CoRR', 'CoRR', 'abs/1606.04199,2016')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Summarize the first section\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: Here are three bullet points summarizing the text:\n",
      "\n",
      "* The authors propose a new neural network architecture called the Transformer, which relies entirely on attention mechanisms and eliminates the need for recurrent or convolutional neural networks.\n",
      "* The Transformer model achieves state-of-the-art results in machine translation tasks, with a BLEU score of 28.4 on the WMT2014 English-to-German translation task and 41.0 on the WMT2014 English-to-French translation task, while requiring significantly less training time and being more parallelizable.\n",
      "* The Transformer architecture is designed to overcome the limitations of traditional recurrent models, which are inherently sequential and cannot be parallelized within training examples, and achieves this by using self-attention mechanisms to draw global dependencies between input and output sequences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  What is self-attention?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: You've already asked this question multiple times. Self-attention is a mechanism that allows a model to attend to all positions in the input sequence simultaneously and weigh their importance. It's a key component of the Transformer model, allowing it to handle long-range dependencies in sequences without recurrence or convolution. \n",
      "\n",
      "To add more detail from the context: \n",
      "\n",
      "* Self-attention layers in the encoder allow each position to attend to all positions in the previous layer.\n",
      "* Self-attention layers in the decoder allow each position to attend to all positions in the decoder up to and including that position, to preserve the auto-regressive property.\n",
      "* The self-attention mechanism is implemented using scaled dot-product attention. \n",
      "\n",
      "The complexity of self-attention is O(n^2 * d), where n is the sequence length and d is the representation dimension. \n",
      "\n",
      "Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 13: INTERACTIVE COORDINATOR LOOP ─────────────────────────────────\n",
    "print(\"🔎 RAG+Agents over Attention Paper. Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_in = input(\"\\nYou: \")\n",
    "    if user_in.lower() in (\"exit\", \"quit\"):\n",
    "        break\n",
    "\n",
    "    # Dispatch to the right agent\n",
    "    response = coord.dispatch(user_in)\n",
    "    print(\"\\nAssistant:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
